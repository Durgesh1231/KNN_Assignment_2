{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9B_EK4Me9ix"
      },
      "outputs": [],
      "source": [
        "# Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN?\n",
        "# - Euclidean distance is the straight-line distance between two points and is computed using the Pythagorean theorem.\n",
        "# - Manhattan distance (also known as L1 distance) is the sum of the absolute differences between the coordinates.\n",
        "# The main difference is that Euclidean distance accounts for the diagonal distance in multi-dimensional space,\n",
        "# while Manhattan distance only considers horizontal and vertical movement.\n",
        "#\n",
        "# How might this difference affect the performance of a KNN classifier or regressor?\n",
        "# - Euclidean distance may work better when the relationship between features is continuous and not constrained to\n",
        "# grid-like structures.\n",
        "# - Manhattan distance may work better in high-dimensional or grid-like spaces, especially when the features are\n",
        "# independent and when there are many outliers.\n",
        "\n",
        "from sklearn.metrics.pairwise import euclidean_distances, manhattan_distances\n",
        "\n",
        "# Example of how to compute Euclidean and Manhattan distance\n",
        "point1 = [[1, 2]]\n",
        "point2 = [[3, 4]]\n",
        "\n",
        "euclidean_dist = euclidean_distances(point1, point2)\n",
        "manhattan_dist = manhattan_distances(point1, point2)\n",
        "\n",
        "print(f\"Euclidean Distance: {euclidean_dist[0][0]:.2f}\")\n",
        "print(f\"Manhattan Distance: {manhattan_dist[0][0]:.2f}\")\n",
        "\n",
        "\n",
        "# Q2. How do you choose the optimal value of k for a KNN classifier or regressor?\n",
        "# The optimal value of k can be chosen by:\n",
        "# 1. Cross-validation: Using techniques like k-fold cross-validation to test different values of k and\n",
        "# selecting the one that minimizes the validation error.\n",
        "# 2. Elbow method: Plotting the error rate for different values of k and selecting the value where the\n",
        "# error rate stabilizes.\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Perform cross-validation for different k values to choose the optimal k\n",
        "import numpy as np\n",
        "k_values = range(1, 21)\n",
        "cv_scores = []\n",
        "\n",
        "# Iterate over different k values\n",
        "for k in k_values:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')\n",
        "    cv_scores.append(np.mean(scores))\n",
        "\n",
        "# Plot the cross-validation scores\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(k_values, cv_scores)\n",
        "plt.xlabel('k Value')\n",
        "plt.ylabel('Cross-Validation Accuracy')\n",
        "plt.title('Optimal K Value Selection')\n",
        "plt.show()\n",
        "\n",
        "# Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor?\n",
        "# - The choice of distance metric determines how \"closeness\" between instances is measured. Different metrics\n",
        "# can perform better or worse depending on the nature of the data:\n",
        "#   - Euclidean distance works well when features are continuous and similar in scale.\n",
        "#   - Manhattan distance is often preferred when features are not as continuous or in grid-like structures.\n",
        "#   - Other metrics (e.g., Minkowski, Cosine similarity) can be used in different problem settings.\n",
        "# You might choose Euclidean distance for problems with naturally continuous data (like image recognition),\n",
        "# while Manhattan distance might be better for problems with independent features or categorical data (e.g.,\n",
        "# in high-dimensional spaces).\n",
        "\n",
        "# Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect\n",
        "# the performance of the model? How might you go about tuning these hyperparameters to improve model performance?\n",
        "# Common hyperparameters:\n",
        "# 1. k (number of neighbors): Determines how many neighbors to consider for classification or regression.\n",
        "#    A small k can lead to overfitting, and a large k can lead to underfitting.\n",
        "# 2. Weights: Determines how much influence each neighbor has (uniform or distance-based).\n",
        "# 3. Distance metric: Affects how the distances between points are computed (Euclidean, Manhattan, etc.).\n",
        "# 4. Algorithm: Can be 'auto', 'ball_tree', 'kd_tree', or 'brute' for nearest neighbor search.\n",
        "# Tuning these hyperparameters can be done using cross-validation (GridSearchCV or RandomizedSearchCV).\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'n_neighbors': [3, 5, 7, 9],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
        "\n",
        "\n",
        "# Q5. How does the size of the training set affect the performance of a KNN classifier or regressor?\n",
        "# The performance of KNN improves with a larger training set because more neighbors are available for prediction,\n",
        "# leading to better generalization. However, increasing the training set size can also increase the computational\n",
        "# complexity and training time.\n",
        "#\n",
        "# Techniques to optimize the size of the training set:\n",
        "# 1. Sampling: Use techniques like stratified sampling or random sampling to ensure a diverse set of data.\n",
        "# 2. Dimensionality reduction: Using methods like PCA to reduce the number of features and thus speed up the training.\n",
        "# 3. Approximate nearest neighbor search: For large datasets, using KD-trees, Ball-trees, or Approximate Nearest\n",
        "# Neighbor (ANN) search can speed up the KNN prediction process.\n",
        "\n",
        "# Q6. What are some potential drawbacks of using KNN as a classifier or regressor?\n",
        "# How might you overcome these drawbacks to improve the performance of the model?\n",
        "# Drawbacks of KNN:\n",
        "# 1. Computational cost: As KNN is a lazy learner, the training phase is fast, but making predictions can be slow,\n",
        "# especially for large datasets.\n",
        "# 2. Sensitivity to irrelevant features: KNN can be sensitive to noise and irrelevant features.\n",
        "# 3. Curse of dimensionality: Performance degrades in high-dimensional spaces.\n",
        "#\n",
        "# Solutions:\n",
        "# 1. Use dimensionality reduction techniques like PCA (Principal Component Analysis).\n",
        "# 2. Use feature selection to identify and retain the most important features.\n",
        "# 3. Use faster nearest-neighbor search techniques like KD-trees or Ball-trees.\n",
        "# 4. Optimize the choice of k and distance metrics through hyperparameter tuning.\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Apply PCA for dimensionality reduction before using KNN\n",
        "pca = PCA(n_components=2)  # Reducing to 2 components\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "accuracy_pca = knn_pca.score(X_test_pca, y_test)\n",
        "\n",
        "print(f\"Accuracy of KNN with PCA: {accuracy_pca:.2f}\")\n"
      ]
    }
  ]
}